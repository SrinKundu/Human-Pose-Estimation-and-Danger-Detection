{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41ffdb0-c340-4870-96c7-1faf0cd489b2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "executionInfo": {
     "elapsed": 666,
     "status": "error",
     "timestamp": 1743625514316,
     "user": {
      "displayName": "Upal Pahari",
      "userId": "17652444065368943002"
     },
     "user_tz": -330
    },
    "id": "f41ffdb0-c340-4870-96c7-1faf0cd489b2",
    "outputId": "707a9682-d79e-4fd1-e453-e5948e3ba840"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Initialize MediaPipe Pose\n",
    "class PoseDetector():\n",
    "    def __init__(self, detectionCon=0.5, trackCon=0.5):\n",
    "        self.mpPose = mp.solutions.pose\n",
    "        self.pose = self.mpPose.Pose(\n",
    "            min_detection_confidence=detectionCon, min_tracking_confidence=trackCon)\n",
    "\n",
    "    def findPose(self, img):\n",
    "        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        self.results = self.pose.process(imgRGB)\n",
    "        return img\n",
    "\n",
    "    def findPosition(self, img):\n",
    "        lmList = []\n",
    "        if self.results.pose_landmarks:\n",
    "            h, w, _ = img.shape\n",
    "\n",
    "            left_hip = self.results.pose_landmarks.landmark[23]\n",
    "            right_hip = self.results.pose_landmarks.landmark[24]\n",
    "            mid_hip_x = (left_hip.x + right_hip.x) / 2\n",
    "            mid_hip_y = (left_hip.y + right_hip.y) / 2\n",
    "\n",
    "            left_shoulder = self.results.pose_landmarks.landmark[11]\n",
    "            right_shoulder = self.results.pose_landmarks.landmark[12]\n",
    "            shoulder_width = abs(left_shoulder.x - right_shoulder.x) + 1e-6\n",
    "\n",
    "            for lm in self.results.pose_landmarks.landmark:\n",
    "                cx = (lm.x - mid_hip_x) / shoulder_width\n",
    "                cy = (lm.y - mid_hip_y) / shoulder_width\n",
    "                cz = lm.z / shoulder_width\n",
    "                lmList.append([cx, cy, cz])\n",
    "\n",
    "        return np.array(lmList).flatten() if lmList else np.zeros(99)\n",
    "\n",
    "def extract_landmarks(video_path, label, max_frames=1000):\n",
    "    if not os.path.exists(video_path):\n",
    "        print(f\"⚠️ ERROR: Video file '{video_path}' not found.\")\n",
    "        return []\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    detector = PoseDetector()\n",
    "    all_landmarks = []\n",
    "\n",
    "    frame_count = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        success, img = cap.read()\n",
    "        if not success or frame_count >= max_frames:\n",
    "            break\n",
    "\n",
    "        img = detector.findPose(img)\n",
    "        lm_data = detector.findPosition(img)\n",
    "\n",
    "        if lm_data is not None and np.any(lm_data):\n",
    "            all_landmarks.append(np.append(lm_data, label))\n",
    "            frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    print(f\"✅ Extracted {frame_count} frames from {video_path}\")\n",
    "    return all_landmarks\n",
    "\n",
    "# Define Activities (Danger = 0, Non-Danger = 1)\n",
    "activities = {\n",
    "    \"fighting.mp4\": 0,\n",
    "    \"Kick1.mp4\": 0,\n",
    "    \"Punch1.mp4\": 0,\n",
    "    \"Shove1.mp4\": 0,\n",
    "    \"slap1.mp4\": 0,\n",
    "    \"Nun1.mp4\": 0,\n",
    "    \"Shoot1.mp4\": 0,\n",
    "    \"sitting.mp4\": 1,\n",
    "    \"walking3.mp4\": 1,\n",
    "    \"jumping.mp4\": 1,\n",
    "    \"bowling1.mp4\": 1\n",
    "}\n",
    "\n",
    "# Extract Data\n",
    "all_data = []\n",
    "for video, label in activities.items():\n",
    "    all_data.extend(extract_landmarks(video, label))\n",
    "\n",
    "# Save Dataset\n",
    "if all_data:\n",
    "    df = pd.DataFrame(all_data)\n",
    "    df.to_csv(\"pose_dataset.csv\", index=False)\n",
    "    print(\"✅ Dataset saved as 'pose_dataset.csv' 🚀\")\n",
    "else:\n",
    "    print(\"⚠️ No data extracted. Check video files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927d63fe-2dd2-4e81-a319-8fb570599530",
   "metadata": {
    "id": "927d63fe-2dd2-4e81-a319-8fb570599530",
    "outputId": "d4859f21-cf6c-4e62-a8c7-bec2d0d6bbd2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "\n",
    "# Load Dataset\n",
    "csv_file = \"pose_dataset.csv\"\n",
    "df = pd.read_csv(csv_file).values\n",
    "X = df[:, :-1]\n",
    "y = df[:, -1]\n",
    "\n",
    "# Prepare Sequences for LSTM\n",
    "TIME_STEPS = 30\n",
    "FEATURES = 99\n",
    "\n",
    "X_seq, y_seq = [], []\n",
    "for i in range(len(X) - TIME_STEPS):\n",
    "    X_seq.append(X[i:i+TIME_STEPS])\n",
    "    y_seq.append(y[i+TIME_STEPS])\n",
    "\n",
    "X_seq, y_seq = np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# Split Train & Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define LSTM Model\n",
    "model = Sequential([\n",
    "    Bidirectional(LSTM(128, return_sequences=True, input_shape=(TIME_STEPS, FEATURES))),\n",
    "    Dropout(0.3),\n",
    "    Bidirectional(LSTM(128)),\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    Dropout(0.3),\n",
    "    Dense(2, activation=\"softmax\")  # 2 Classes: Danger (0) & Non-Danger (1)\n",
    "])\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"sparse_categorical_accuracy\"])\n",
    "\n",
    "# Train Model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_test, y_test))\n",
    "\n",
    "# Save Model\n",
    "model.save(\"lstm_activity_model.h5\")\n",
    "print(\"✅ Model trained & saved as 'lstm_activity_model.h5'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2374e3ec-b98c-43eb-b0f9-02a1e58cef0e",
   "metadata": {
    "id": "2374e3ec-b98c-43eb-b0f9-02a1e58cef0e",
    "outputId": "36acd9c5-d643-4891-ab78-7955f2b12546"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load trained model\n",
    "model = tf.keras.models.load_model(\"lstm_activity_model.h5\")\n",
    "\n",
    "# Labels\n",
    "actions = [\"DANGER\", \"SAFE\"]\n",
    "colors = [(0, 0, 255), (0, 255, 0)]  # RED (Danger) & GREEN (Non-Danger)\n",
    "\n",
    "# Load Test Video\n",
    "cap = cv2.VideoCapture(\"slap1 (1).mp4\")\n",
    "detector = PoseDetector()\n",
    "sequence = []\n",
    "\n",
    "cv2.namedWindow(\"Activity Recognition\", cv2.WINDOW_NORMAL)\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    img = detector.findPose(img)\n",
    "    lm_data = detector.findPosition(img)\n",
    "\n",
    "    if lm_data is not None and np.any(lm_data):\n",
    "        sequence.append(lm_data)\n",
    "        if len(sequence) > TIME_STEPS:\n",
    "            sequence.pop(0)\n",
    "\n",
    "        if len(sequence) == TIME_STEPS:\n",
    "            input_seq = np.expand_dims(np.array(sequence), axis=0)\n",
    "            prediction = np.argmax(model.predict(input_seq), axis=1)[0]\n",
    "            label = actions[prediction]\n",
    "            color = colors[prediction]\n",
    "\n",
    "            cv2.rectangle(img, (20, 30), (250, 100), color, -1)  # Display box\n",
    "            cv2.putText(img, label, (50, 80),\n",
    "                        cv2.FONT_HERSHEY_COMPLEX, 1.2, (255, 255, 255), 3)\n",
    "\n",
    "    img_resized = cv2.resize(img, (800, 600))\n",
    "    cv2.imshow(\"Activity Recognition\", img_resized)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188122d4-eab6-4217-a34f-fcc236d8a197",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 477
    },
    "executionInfo": {
     "elapsed": 283,
     "status": "error",
     "timestamp": 1743587741288,
     "user": {
      "displayName": "Upal Pahari",
      "userId": "17652444065368943002"
     },
     "user_tz": -330
    },
    "id": "188122d4-eab6-4217-a34f-fcc236d8a197",
    "outputId": "c129cb1b-555d-473a-82b0-7299b31485b1"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load trained model\n",
    "model = tf.keras.models.load_model(\"lstm_activity_model.h5\")\n",
    "\n",
    "# Labels\n",
    "actions = [\"DANGER\", \"SAFE\"]\n",
    "colors = [(0, 0, 255), (0, 255, 0)]  # RED (Danger) & GREEN (Non-Danger)\n",
    "\n",
    "# Load Test Video\n",
    "cap = cv2.VideoCapture(\"test_video.mp4\")\n",
    "detector = PoseDetector()\n",
    "sequence = []\n",
    "\n",
    "cv2.namedWindow(\"Activity Recognition\", cv2.WINDOW_NORMAL)\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    img = detector.findPose(img)\n",
    "    lm_data = detector.findPosition(img)\n",
    "\n",
    "    if lm_data is not None and np.any(lm_data):\n",
    "        sequence.append(lm_data)\n",
    "        if len(sequence) > TIME_STEPS:\n",
    "            sequence.pop(0)\n",
    "\n",
    "        if len(sequence) == TIME_STEPS:\n",
    "            input_seq = np.expand_dims(np.array(sequence), axis=0)\n",
    "            prediction = np.argmax(model.predict(input_seq), axis=1)[0]\n",
    "            label = actions[prediction]\n",
    "            color = colors[prediction]\n",
    "\n",
    "            # Adjusted smaller rectangle & font\n",
    "            cv2.rectangle(img, (20, 20), (180, 60), color, -1)  # Smaller box\n",
    "            cv2.putText(img, label, (30, 50),\n",
    "                        cv2.FONT_HERSHEY_COMPLEX, 0.8, (255, 255, 255), 2)  # Smaller font\n",
    "\n",
    "    img_resized = cv2.resize(img, (800, 600))\n",
    "    cv2.imshow(\"Activity Recognition\", img_resized)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce957837-931f-4b7e-8305-9fc6ecd73eef",
   "metadata": {
    "id": "ce957837-931f-4b7e-8305-9fc6ecd73eef"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
