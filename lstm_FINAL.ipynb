{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f41ffdb0-c340-4870-96c7-1faf0cd489b2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "executionInfo": {
     "elapsed": 666,
     "status": "error",
     "timestamp": 1743625514316,
     "user": {
      "displayName": "Upal Pahari",
      "userId": "17652444065368943002"
     },
     "user_tz": -330
    },
    "id": "f41ffdb0-c340-4870-96c7-1faf0cd489b2",
    "outputId": "707a9682-d79e-4fd1-e453-e5948e3ba840"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extracted 54 frames from fighting.mp4\n",
      "✅ Extracted 55 frames from Kick1.mp4\n",
      "✅ Extracted 280 frames from Punch1.mp4\n",
      "✅ Extracted 122 frames from Shove1.mp4\n",
      "✅ Extracted 54 frames from slap1.mp4\n",
      "✅ Extracted 146 frames from Nun1.mp4\n",
      "✅ Extracted 78 frames from Shoot1.mp4\n",
      "✅ Extracted 910 frames from sitting.mp4\n",
      "✅ Extracted 1000 frames from walking3.mp4\n",
      "✅ Extracted 117 frames from jumping.mp4\n",
      "✅ Extracted 443 frames from bowling1.mp4\n",
      "✅ Dataset saved as 'pose_dataset.csv' 🚀\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Initialize MediaPipe Pose\n",
    "class PoseDetector():\n",
    "    def __init__(self, detectionCon=0.5, trackCon=0.5):\n",
    "        self.mpPose = mp.solutions.pose\n",
    "        self.pose = self.mpPose.Pose(\n",
    "            min_detection_confidence=detectionCon, min_tracking_confidence=trackCon)\n",
    "\n",
    "    def findPose(self, img):\n",
    "        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        self.results = self.pose.process(imgRGB)\n",
    "        return img\n",
    "\n",
    "    def findPosition(self, img):\n",
    "        lmList = []\n",
    "        if self.results.pose_landmarks:\n",
    "            h, w, _ = img.shape\n",
    "\n",
    "            left_hip = self.results.pose_landmarks.landmark[23]\n",
    "            right_hip = self.results.pose_landmarks.landmark[24]\n",
    "            mid_hip_x = (left_hip.x + right_hip.x) / 2\n",
    "            mid_hip_y = (left_hip.y + right_hip.y) / 2\n",
    "\n",
    "            left_shoulder = self.results.pose_landmarks.landmark[11]\n",
    "            right_shoulder = self.results.pose_landmarks.landmark[12]\n",
    "            shoulder_width = abs(left_shoulder.x - right_shoulder.x) + 1e-6\n",
    "\n",
    "            for lm in self.results.pose_landmarks.landmark:\n",
    "                cx = (lm.x - mid_hip_x) / shoulder_width\n",
    "                cy = (lm.y - mid_hip_y) / shoulder_width\n",
    "                cz = lm.z / shoulder_width\n",
    "                lmList.append([cx, cy, cz])\n",
    "\n",
    "        return np.array(lmList).flatten() if lmList else np.zeros(99)\n",
    "\n",
    "def extract_landmarks(video_path, label, max_frames=1000):\n",
    "    if not os.path.exists(video_path):\n",
    "        print(f\"⚠️ ERROR: Video file '{video_path}' not found.\")\n",
    "        return []\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    detector = PoseDetector()\n",
    "    all_landmarks = []\n",
    "\n",
    "    frame_count = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        success, img = cap.read()\n",
    "        if not success or frame_count >= max_frames:\n",
    "            break\n",
    "\n",
    "        img = detector.findPose(img)\n",
    "        lm_data = detector.findPosition(img)\n",
    "\n",
    "        if lm_data is not None and np.any(lm_data):\n",
    "            all_landmarks.append(np.append(lm_data, label))\n",
    "            frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    print(f\"✅ Extracted {frame_count} frames from {video_path}\")\n",
    "    return all_landmarks\n",
    "\n",
    "# Define Activities (Danger = 0, Non-Danger = 1)\n",
    "activities = {\n",
    "    \"fighting.mp4\": 0,\n",
    "    \"Kick1.mp4\": 0,\n",
    "    \"Punch1.mp4\": 0,\n",
    "    \"Shove1.mp4\": 0,\n",
    "    \"slap1.mp4\": 0,\n",
    "    \"Nun1.mp4\": 0,\n",
    "    \"Shoot1.mp4\": 0,\n",
    "    \"sitting.mp4\": 1,\n",
    "    \"walking3.mp4\": 1,\n",
    "    \"jumping.mp4\": 1,\n",
    "    \"bowling1.mp4\": 1\n",
    "}\n",
    "\n",
    "# Extract Data\n",
    "all_data = []\n",
    "for video, label in activities.items():\n",
    "    all_data.extend(extract_landmarks(video, label))\n",
    "\n",
    "# Save Dataset\n",
    "if all_data:\n",
    "    df = pd.DataFrame(all_data)\n",
    "    df.to_csv(\"pose_dataset.csv\", index=False)\n",
    "    print(\"✅ Dataset saved as 'pose_dataset.csv' 🚀\")\n",
    "else:\n",
    "    print(\"⚠️ No data extracted. Check video files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "927d63fe-2dd2-4e81-a319-8fb570599530",
   "metadata": {
    "id": "927d63fe-2dd2-4e81-a319-8fb570599530",
    "outputId": "d4859f21-cf6c-4e62-a8c7-bec2d0d6bbd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\srinj\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 85ms/step - loss: 0.1579 - sparse_categorical_accuracy: 0.9204 - val_loss: 0.0052 - val_sparse_categorical_accuracy: 0.9985\n",
      "Epoch 2/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 67ms/step - loss: 0.0062 - sparse_categorical_accuracy: 0.9981 - val_loss: 0.0037 - val_sparse_categorical_accuracy: 0.9985\n",
      "Epoch 3/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 92ms/step - loss: 9.8814e-04 - sparse_categorical_accuracy: 0.9996 - val_loss: 0.0034 - val_sparse_categorical_accuracy: 0.9985\n",
      "Epoch 4/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 93ms/step - loss: 3.3484e-04 - sparse_categorical_accuracy: 0.9999 - val_loss: 0.0027 - val_sparse_categorical_accuracy: 0.9985\n",
      "Epoch 5/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 95ms/step - loss: 3.6539e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 6.0775e-04 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 6/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 98ms/step - loss: 0.0022 - sparse_categorical_accuracy: 0.9987 - val_loss: 0.0017 - val_sparse_categorical_accuracy: 0.9985\n",
      "Epoch 7/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 90ms/step - loss: 5.7581e-04 - sparse_categorical_accuracy: 0.9996 - val_loss: 0.0038 - val_sparse_categorical_accuracy: 0.9985\n",
      "Epoch 8/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 95ms/step - loss: 6.9826e-05 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0034 - val_sparse_categorical_accuracy: 0.9985\n",
      "Epoch 9/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 97ms/step - loss: 1.0525e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0045 - val_sparse_categorical_accuracy: 0.9985\n",
      "Epoch 10/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 97ms/step - loss: 4.1513e-05 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0050 - val_sparse_categorical_accuracy: 0.9985\n",
      "Epoch 11/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 92ms/step - loss: 2.8026e-05 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0051 - val_sparse_categorical_accuracy: 0.9985\n",
      "Epoch 12/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 88ms/step - loss: 3.4451e-05 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0059 - val_sparse_categorical_accuracy: 0.9985\n",
      "Epoch 13/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 89ms/step - loss: 2.3617e-05 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0060 - val_sparse_categorical_accuracy: 0.9985\n",
      "Epoch 14/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 88ms/step - loss: 4.3357e-05 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0056 - val_sparse_categorical_accuracy: 0.9985\n",
      "Epoch 15/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 88ms/step - loss: 1.6330e-05 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0057 - val_sparse_categorical_accuracy: 0.9985\n",
      "Epoch 16/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 82ms/step - loss: 1.2679e-05 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0057 - val_sparse_categorical_accuracy: 0.9985\n",
      "Epoch 17/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 96ms/step - loss: 2.2235e-05 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0053 - val_sparse_categorical_accuracy: 0.9985\n",
      "Epoch 18/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 96ms/step - loss: 1.3272e-05 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0053 - val_sparse_categorical_accuracy: 0.9985\n",
      "Epoch 19/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 100ms/step - loss: 1.0778e-05 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0061 - val_sparse_categorical_accuracy: 0.9985\n",
      "Epoch 20/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 107ms/step - loss: 1.0099e-05 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0065 - val_sparse_categorical_accuracy: 0.9985\n",
      "Epoch 21/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 103ms/step - loss: 1.0882e-05 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0065 - val_sparse_categorical_accuracy: 0.9985\n",
      "Epoch 22/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 90ms/step - loss: 5.5651e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0068 - val_sparse_categorical_accuracy: 0.9985\n",
      "Epoch 23/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 91ms/step - loss: 7.7568e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0069 - val_sparse_categorical_accuracy: 0.9985\n",
      "Epoch 24/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 101ms/step - loss: 5.8610e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0071 - val_sparse_categorical_accuracy: 0.9985\n",
      "Epoch 25/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 87ms/step - loss: 1.5806e-05 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0081 - val_sparse_categorical_accuracy: 0.9985\n",
      "Epoch 26/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 89ms/step - loss: 1.3296e-05 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0077 - val_sparse_categorical_accuracy: 0.9985\n",
      "Epoch 27/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 92ms/step - loss: 6.8498e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0089 - val_sparse_categorical_accuracy: 0.9985\n",
      "Epoch 28/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 88ms/step - loss: 3.3932e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0093 - val_sparse_categorical_accuracy: 0.9985\n",
      "Epoch 29/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 93ms/step - loss: 5.1650e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0090 - val_sparse_categorical_accuracy: 0.9985\n",
      "Epoch 30/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 111ms/step - loss: 2.4279e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0073 - val_sparse_categorical_accuracy: 0.9985\n",
      "Epoch 31/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 106ms/step - loss: 3.2436e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0079 - val_sparse_categorical_accuracy: 0.9985\n",
      "Epoch 32/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 91ms/step - loss: 2.2092e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0082 - val_sparse_categorical_accuracy: 0.9985\n",
      "Epoch 33/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 86ms/step - loss: 2.5364e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0084 - val_sparse_categorical_accuracy: 0.9985\n",
      "Epoch 34/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 87ms/step - loss: 2.0836e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0085 - val_sparse_categorical_accuracy: 0.9985\n",
      "Epoch 35/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 87ms/step - loss: 3.4664e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0086 - val_sparse_categorical_accuracy: 0.9985\n",
      "Epoch 36/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 87ms/step - loss: 4.0921e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0089 - val_sparse_categorical_accuracy: 0.9985\n",
      "Epoch 37/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 87ms/step - loss: 1.8006e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0091 - val_sparse_categorical_accuracy: 0.9985\n",
      "Epoch 38/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 86ms/step - loss: 3.1008e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0096 - val_sparse_categorical_accuracy: 0.9985\n",
      "Epoch 39/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 84ms/step - loss: 1.6556e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0097 - val_sparse_categorical_accuracy: 0.9985\n",
      "Epoch 40/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 84ms/step - loss: 1.2737e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0097 - val_sparse_categorical_accuracy: 0.9985\n",
      "Epoch 41/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 84ms/step - loss: 1.7204e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0097 - val_sparse_categorical_accuracy: 0.9985\n",
      "Epoch 42/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 87ms/step - loss: 1.6698e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0098 - val_sparse_categorical_accuracy: 0.9985\n",
      "Epoch 43/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 97ms/step - loss: 4.5068e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0099 - val_sparse_categorical_accuracy: 0.9985\n",
      "Epoch 44/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 99ms/step - loss: 5.0413e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0103 - val_sparse_categorical_accuracy: 0.9985\n",
      "Epoch 45/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 96ms/step - loss: 1.9394e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0104 - val_sparse_categorical_accuracy: 0.9985\n",
      "Epoch 46/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 96ms/step - loss: 1.5184e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0105 - val_sparse_categorical_accuracy: 0.9985\n",
      "Epoch 47/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 86ms/step - loss: 7.6111e-07 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0107 - val_sparse_categorical_accuracy: 0.9985\n",
      "Epoch 48/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 128ms/step - loss: 1.0587e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0107 - val_sparse_categorical_accuracy: 0.9985\n",
      "Epoch 49/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 95ms/step - loss: 1.1193e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0107 - val_sparse_categorical_accuracy: 0.9985\n",
      "Epoch 50/50\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 91ms/step - loss: 1.7179e-06 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.0108 - val_sparse_categorical_accuracy: 0.9985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model trained & saved as 'lstm_activity_model.h5'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "\n",
    "# Load Dataset\n",
    "csv_file = \"pose_dataset.csv\"\n",
    "df = pd.read_csv(csv_file).values\n",
    "X = df[:, :-1]\n",
    "y = df[:, -1]\n",
    "\n",
    "# Prepare Sequences for LSTM\n",
    "TIME_STEPS = 30\n",
    "FEATURES = 99\n",
    "\n",
    "X_seq, y_seq = [], []\n",
    "for i in range(len(X) - TIME_STEPS):\n",
    "    X_seq.append(X[i:i+TIME_STEPS])\n",
    "    y_seq.append(y[i+TIME_STEPS])\n",
    "\n",
    "X_seq, y_seq = np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# Split Train & Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define LSTM Model\n",
    "model = Sequential([\n",
    "    Bidirectional(LSTM(128, return_sequences=True, input_shape=(TIME_STEPS, FEATURES))),\n",
    "    Dropout(0.3),\n",
    "    Bidirectional(LSTM(128)),\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    Dropout(0.3),\n",
    "    Dense(2, activation=\"softmax\")  # 2 Classes: Danger (0) & Non-Danger (1)\n",
    "])\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"sparse_categorical_accuracy\"])\n",
    "\n",
    "# Train Model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_test, y_test))\n",
    "\n",
    "# Save Model\n",
    "model.save(\"lstm_activity_model.h5\")\n",
    "print(\"✅ Model trained & saved as 'lstm_activity_model.h5'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2374e3ec-b98c-43eb-b0f9-02a1e58cef0e",
   "metadata": {
    "id": "2374e3ec-b98c-43eb-b0f9-02a1e58cef0e",
    "outputId": "36acd9c5-d643-4891-ab78-7955f2b12546"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 858ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load trained model\n",
    "model = tf.keras.models.load_model(\"lstm_activity_model.h5\")\n",
    "\n",
    "# Labels\n",
    "actions = [\"DANGER\", \"SAFE\"]\n",
    "colors = [(0, 0, 255), (0, 255, 0)]  # RED (Danger) & GREEN (Non-Danger)\n",
    "\n",
    "# Load Test Video\n",
    "cap = cv2.VideoCapture(\"slap1 (1).mp4\")\n",
    "detector = PoseDetector()\n",
    "sequence = []\n",
    "\n",
    "cv2.namedWindow(\"Activity Recognition\", cv2.WINDOW_NORMAL)\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    img = detector.findPose(img)\n",
    "    lm_data = detector.findPosition(img)\n",
    "\n",
    "    if lm_data is not None and np.any(lm_data):\n",
    "        sequence.append(lm_data)\n",
    "        if len(sequence) > TIME_STEPS:\n",
    "            sequence.pop(0)\n",
    "\n",
    "        if len(sequence) == TIME_STEPS:\n",
    "            input_seq = np.expand_dims(np.array(sequence), axis=0)\n",
    "            prediction = np.argmax(model.predict(input_seq), axis=1)[0]\n",
    "            label = actions[prediction]\n",
    "            color = colors[prediction]\n",
    "\n",
    "            cv2.rectangle(img, (20, 30), (250, 100), color, -1)  # Display box\n",
    "            cv2.putText(img, label, (50, 80),\n",
    "                        cv2.FONT_HERSHEY_COMPLEX, 1.2, (255, 255, 255), 3)\n",
    "\n",
    "    img_resized = cv2.resize(img, (800, 600))\n",
    "    cv2.imshow(\"Activity Recognition\", img_resized)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188122d4-eab6-4217-a34f-fcc236d8a197",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 477
    },
    "executionInfo": {
     "elapsed": 283,
     "status": "error",
     "timestamp": 1743587741288,
     "user": {
      "displayName": "Upal Pahari",
      "userId": "17652444065368943002"
     },
     "user_tz": -330
    },
    "id": "188122d4-eab6-4217-a34f-fcc236d8a197",
    "outputId": "c129cb1b-555d-473a-82b0-7299b31485b1"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load trained model\n",
    "model = tf.keras.models.load_model(\"lstm_activity_model.h5\")\n",
    "\n",
    "# Labels\n",
    "actions = [\"DANGER\", \"SAFE\"]\n",
    "colors = [(0, 0, 255), (0, 255, 0)]  # RED (Danger) & GREEN (Non-Danger)\n",
    "\n",
    "# Load Test Video\n",
    "cap = cv2.VideoCapture(\"test_video.mp4\")\n",
    "detector = PoseDetector()\n",
    "sequence = []\n",
    "\n",
    "cv2.namedWindow(\"Activity Recognition\", cv2.WINDOW_NORMAL)\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    img = detector.findPose(img)\n",
    "    lm_data = detector.findPosition(img)\n",
    "\n",
    "    if lm_data is not None and np.any(lm_data):\n",
    "        sequence.append(lm_data)\n",
    "        if len(sequence) > TIME_STEPS:\n",
    "            sequence.pop(0)\n",
    "\n",
    "        if len(sequence) == TIME_STEPS:\n",
    "            input_seq = np.expand_dims(np.array(sequence), axis=0)\n",
    "            prediction = np.argmax(model.predict(input_seq), axis=1)[0]\n",
    "            label = actions[prediction]\n",
    "            color = colors[prediction]\n",
    "\n",
    "            # Adjusted smaller rectangle & font\n",
    "            cv2.rectangle(img, (20, 20), (180, 60), color, -1)  # Smaller box\n",
    "            cv2.putText(img, label, (30, 50),\n",
    "                        cv2.FONT_HERSHEY_COMPLEX, 0.8, (255, 255, 255), 2)  # Smaller font\n",
    "\n",
    "    img_resized = cv2.resize(img, (800, 600))\n",
    "    cv2.imshow(\"Activity Recognition\", img_resized)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce957837-931f-4b7e-8305-9fc6ecd73eef",
   "metadata": {
    "id": "ce957837-931f-4b7e-8305-9fc6ecd73eef"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
